{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d9ef1-6bf6-4ee1-85da-1ee2c5457125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, losses, backend, activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "from custom_utils import validation_setup, MLP_model, PHL_model\n",
    "\n",
    "disable_eager_execution()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "pd.set_option(\"display.max_rows\", 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a5b87-293f-4ec7-8205-3c6f30de72c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38541ab-da9e-4794-8af1-caa299205b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/mimic'\n",
    "output_path = '../output/mimic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cef59-d4da-4a58-a80e-8ffed9f911a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'/df_feature.pickle', 'rb') as f:\n",
    "    df_feature = pickle.load(f).fillna(0)\n",
    "\n",
    "with open(data_path+'/df_label.pickle', 'rb') as f:\n",
    "    df_label = pickle.load(f).fillna(0).drop('C_HUA', axis=1) # not predicting disease HUA for most of the providers didn't have corresponding diagnoses records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6a1cc-915a-4d40-a24e-d02654bda1c4",
   "metadata": {},
   "source": [
    "---\n",
    "### setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f78a7-2cff-400d-8cf8-30ee063c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider_ids = df_feature.index.get_level_values(0).unique()\n",
    "\n",
    "n_folds, n_providers_per_fold = 11, 5\n",
    "n_providers = n_folds * n_providers_per_fold \n",
    "n_inputs = df_feature.shape[1]\n",
    "n_labels = df_label.shape[1]\n",
    "\n",
    "col_labels = df_label.columns\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 40\n",
    "epsilon = 1e-7\n",
    "\n",
    "# common hyper-parameters (tuned on MLP)\n",
    "layer_shape = [200, 50, 30]\n",
    "wpa_list = np.arange(0,11)/2\n",
    "used_reg = 8e-5\n",
    "used_wpa = 2\n",
    "\n",
    "# PHL hyper-parameters\n",
    "gk_l1l2, gb_l1l2 = 1e-2, 3e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087f0a0-10dd-45f2-85e1-620b5f2ede10",
   "metadata": {},
   "source": [
    "---\n",
    "### train test split, $2\\times k$ folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74a09c-1717-4074-bb48-75ea82f27dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, valid_list = validation_setup(n_folds, n_providers_per_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26bcd03-dd90-4058-8e7e-979188ebb765",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 1.1 Baseline\n",
    "### 1.1 train by folds (LORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b626899-ac0f-4118-98f4-e9f54d199b0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg_name = 'MLP'\n",
    "\n",
    "if not os.path.exists(output_path + '/' + alg_name):\n",
    "    os.mkdir(output_path + '/' + alg_name)\n",
    "        \n",
    "for j_fold in range(n_folds):\n",
    "\n",
    "    for i_fold in range(2):\n",
    "\n",
    "            # select training and validation sets\n",
    "            train_pids = provider_ids[train_list[j_fold][i_fold]]\n",
    "            train_inputs = df_feature.loc[train_pids]\n",
    "            train_labels = df_label.loc[train_pids]\n",
    "\n",
    "            valid_pids = provider_ids[valid_list[j_fold]]\n",
    "            valid_inputs = df_feature.loc[valid_pids]\n",
    "            valid_labels = df_label.loc[valid_pids]\n",
    "\n",
    "            print(j_fold, i_fold, train_pids, valid_pids, train_inputs.shape,  valid_inputs.shape)\n",
    "            \n",
    "            model = MLP_model(\n",
    "                params={'n_inputs': n_inputs,\n",
    "                        'n_labels': n_labels,\n",
    "                        'layer_shape': layer_shape,\n",
    "                        'common_reg': used_reg,},\n",
    "            ) \n",
    "            \n",
    "            model.fit(\n",
    "                [train_inputs, train_labels], [], \n",
    "                batch_size=batch_size, \n",
    "                epochs=n_iters,\n",
    "                shuffle=True,\n",
    "                workers=40,\n",
    "                use_multiprocessing=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "            \n",
    "            valid_pred = model.predict([valid_inputs, np.zeros(valid_labels.shape)])\n",
    "\n",
    "            with open(output_path + '/%s/pred_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "                pickle.dump(valid_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b64e49-68a7-443e-a516-733dba13efea",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. the propensity-harnessed learning (PHL) - step 1 & 2 seperated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce09ab-2988-4f5b-94f6-3b41db58fde7",
   "metadata": {},
   "source": [
    "### 2.1 step 1 train individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fe3cf-07e2-4722-8d41-86a89fa82062",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg_name = 'ind'\n",
    "\n",
    "if not os.path.exists(output_path + '/' + alg_name):\n",
    "    os.mkdir(output_path + '/' + alg_name)\n",
    "    \n",
    "for i_train, train_pid in enumerate(provider_ids):\n",
    "    \n",
    "    print('train',i_train, train_pid)\n",
    "    \n",
    "    ind_model = MLP_model(\n",
    "        params={'n_inputs': n_inputs,\n",
    "                'n_labels': n_labels,\n",
    "                'layer_shape': layer_shape,\n",
    "                'common_reg': used_reg,},\n",
    "    ) \n",
    "\n",
    "    ind_model.fit(\n",
    "        [df_feature.loc[train_pid], df_label.loc[train_pid]], [], \n",
    "        batch_size=batch_size, \n",
    "        epochs=n_iters,\n",
    "        shuffle=True,\n",
    "        workers=40,\n",
    "        use_multiprocessing=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "    \n",
    "    for valid_pid in provider_ids:\n",
    "        \n",
    "        valid_pred = ind_model.predict([df_feature.loc[valid_pid], \n",
    "                                        np.zeros(df_label.loc[valid_pid].shape)])\n",
    "        \n",
    "        with open(output_path + '/%s/pred_train_on_%s_valid_on_%s.pickle' % (alg_name, train_pid, valid_pid), 'wb') as f:\n",
    "                pickle.dump(valid_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9b5ee-5157-44ab-aa23-2516bf9cc7d3",
   "metadata": {},
   "source": [
    "### 2.2  step 2 by folds (LORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe79e7b-e8ba-4f7f-b404-e195b5d5f1f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for wpa in wpa_list:\n",
    "\n",
    "    alg_name = 'PHL-w=%.1f' % (wpa)\n",
    "        \n",
    "    if not os.path.exists(output_path + '/' + alg_name):\n",
    "        os.mkdir(output_path + '/' + alg_name)\n",
    "        \n",
    "    for j_fold in range(n_folds):\n",
    "\n",
    "        for i_fold in range(2):\n",
    "                \n",
    "            if os.path.exists(output_path + '/%s/pred_g_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold)):\n",
    "                continue\n",
    "\n",
    "            # select training and validation sets\n",
    "            train_pids = provider_ids[train_list[j_fold][i_fold]]\n",
    "            n_train_providers = len(train_pids)\n",
    "            train_inputs = df_feature.loc[train_pids]\n",
    "            train_labels = df_label.loc[train_pids]\n",
    "            train_flags = pd.concat([pd.DataFrame(index=df_feature.loc[pid].index, columns=[pid], data=1) for pid in train_pids]).fillna(0)\n",
    "\n",
    "            valid_pids = provider_ids[valid_list[j_fold]]\n",
    "            valid_inputs = df_feature.loc[valid_pids]\n",
    "            valid_labels = df_label.loc[valid_pids]\n",
    "\n",
    "            print(j_fold, i_fold, train_pids, valid_pids, train_inputs.shape,  valid_inputs.shape)\n",
    "\n",
    "            # step 1, get predictions from individual classsifiers\n",
    "            train_fk_inputs = []\n",
    "            for train_pid in train_pids:\n",
    "                ind_fk_inputs = []\n",
    "                for valid_pid in train_pids:\n",
    "                    with open(output_path + '/ind/pred_train_on_%s_valid_on_%s.pickle' % (train_pid, valid_pid), 'rb') as f:\n",
    "                        ind_fk_inputs.append(pickle.load(f))\n",
    "                ind_fk_inputs = np.vstack(ind_fk_inputs)\n",
    "                train_fk_inputs.append(ind_fk_inputs[:, np.newaxis, :])\n",
    "            train_fk_inputs = np.hstack(train_fk_inputs)\n",
    "\n",
    "            valid_fk_inputs = []\n",
    "            for train_pid in train_pids:\n",
    "                ind_fk_inputs = []\n",
    "                for valid_pid in valid_pids:\n",
    "                    with open(output_path + '/ind/pred_train_on_%s_valid_on_%s.pickle' % (train_pid, valid_pid), 'rb') as f:\n",
    "                        ind_fk_inputs.append(pickle.load(f))\n",
    "                ind_fk_inputs = np.vstack(ind_fk_inputs)\n",
    "                valid_fk_inputs.append(ind_fk_inputs[:, np.newaxis, :])\n",
    "            valid_fk_inputs = np.hstack(valid_fk_inputs)\n",
    "            \n",
    "            # step 2, train PHL model\n",
    "            PHL_model = PHL_model(\n",
    "                 params={'n_inputs': n_inputs,\n",
    "                         'n_labels': n_labels,\n",
    "                         'layer_shape': layer_shape,\n",
    "                         'common_reg': used_reg,    \n",
    "                         'n_train_providers': n_train_providers,\n",
    "                         'gk_l1l2': gk_l1l2,\n",
    "                         'gb_l1l2': gb_l1l2},\n",
    "                w_pa=wpa,\n",
    "            ) \n",
    "\n",
    "            print('learning PHL model')\n",
    "\n",
    "            t0=time.time()\n",
    "                \n",
    "            PHL_model.fit(\n",
    "                [train_inputs, train_fk_inputs, train_labels, train_flags], [], \n",
    "                batch_size=batch_size, \n",
    "                epochs=n_iters,\n",
    "                shuffle=True,\n",
    "                workers=40,\n",
    "                use_multiprocessing=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "            t1 = time.time()\n",
    "                    \n",
    "            print('duration: ', t1-t0, 's')\n",
    "                    \n",
    "            # prediction\n",
    "            train_pred = PHL_model.predict([train_inputs, \n",
    "                                             train_fk_inputs,\n",
    "                                             np.zeros(train_labels.shape),\n",
    "                                             np.zeros([train_labels.shape[0], n_train_providers])])\n",
    "\n",
    "            train_f_outputs, train_g_outputs = train_pred\n",
    "\n",
    "            valid_pred = PHL_model.predict([valid_inputs, \n",
    "                                             valid_fk_inputs,\n",
    "                                             np.zeros(valid_labels.shape),\n",
    "                                             np.zeros([valid_labels.shape[0], n_train_providers])])\n",
    "\n",
    "            valid_f_outputs, valid_g_outputs = valid_pred\n",
    "\n",
    "            # storage\n",
    "            with open(output_path + '/%s/train_pred_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "                pickle.dump(train_f_outputs, f)\n",
    "            with open(output_path + '/%s/train_pred_g_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "                pickle.dump(train_g_outputs, f)\n",
    "            with open(output_path + '/%s/pred_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "                pickle.dump(valid_f_outputs, f)\n",
    "            with open(output_path + '/%s/pred_g_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:        \n",
    "                pickle.dump(valid_g_outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00abbb-c31a-45ca-ae25-3faf3160f4a7",
   "metadata": {},
   "source": [
    "### 2.3 step 2 using all physicians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84452445-3c46-4eb4-a2cc-1469f6ced78d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg_name = 'PHL-all-w=%.1f' % (used_wpa)\n",
    "        \n",
    "if not os.path.exists(output_path + '/' + alg_name):\n",
    "    os.mkdir(output_path + '/' + alg_name)         \n",
    "\n",
    "# select training and validation sets\n",
    "train_pids = provider_ids\n",
    "n_train_providers = len(train_pids)\n",
    "train_inputs = df_feature.loc[train_pids]\n",
    "train_labels = df_label.loc[train_pids]\n",
    "train_flags = pd.concat([pd.DataFrame(index=df_feature.loc[pid].index, columns=[pid], data=1) for pid in train_pids]).fillna(0)\n",
    "\n",
    "print(train_pids, train_inputs.shape)\n",
    "\n",
    "# step 1, get predictions from individual classsifiers\n",
    "train_fk_inputs = []\n",
    "for train_pid in train_pids:\n",
    "    ind_fk_inputs = []\n",
    "    for valid_pid in train_pids:\n",
    "        with open(output_path + '/ind/pred_train_on_%s_valid_on_%s.pickle' % (train_pid, valid_pid), 'rb') as f:\n",
    "            ind_fk_inputs.append(pickle.load(f))\n",
    "    ind_fk_inputs = np.vstack(ind_fk_inputs)\n",
    "    train_fk_inputs.append(ind_fk_inputs[:, np.newaxis, :])\n",
    "train_fk_inputs = np.hstack(train_fk_inputs)\n",
    "\n",
    "# step 2, train PHL model\n",
    "PHL_model = PHL_model(\n",
    "        params={'n_inputs': n_inputs,\n",
    "                'n_labels': n_labels,\n",
    "                'layer_shape': layer_shape,\n",
    "                'common_reg': used_reg,    \n",
    "                'n_train_providers': n_train_providers,\n",
    "                'gk_l1l2': gk_l1l2,\n",
    "                'gb_l1l2': gb_l1l2},\n",
    "    w_pa=used_wpa,\n",
    ") \n",
    "\n",
    "print('learning PHL model')\n",
    "\n",
    "t0=time.time()\n",
    "                \n",
    "PHL_model.fit(\n",
    "    [train_inputs, train_fk_inputs, train_labels, train_flags], [], \n",
    "    batch_size=batch_size, \n",
    "    epochs=n_iters,\n",
    "    shuffle=True,\n",
    "    workers=40,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "                    \n",
    "print('duration: ', t1-t0, 's')\n",
    "                    \n",
    "# prediction\n",
    "train_pred = PHL_model.predict([train_inputs, \n",
    "                                 train_fk_inputs,\n",
    "                                 np.zeros(train_labels.shape),\n",
    "                                 np.zeros([train_labels.shape[0], n_train_providers])])\n",
    "\n",
    "train_f_outputs, train_g_outputs = train_pred\n",
    "\n",
    "# storage\n",
    "with open(output_path + '/%s/train_pred.pickle' % (alg_name), 'wb') as f:\n",
    "    pickle.dump(train_f_outputs, f)\n",
    "with open(output_path + '/%s/train_pred_g.pickle' % (alg_name), 'wb') as f:\n",
    "    pickle.dump(train_g_outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c29ea7-f209-40d0-b8c8-bb757f651bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_multihead_layer = PHL_model.get_layer(name='g_multihead_layer')\n",
    "with open(output_path + '/%s/train_kernel.pickle' % (alg_name), 'wb') as f:\n",
    "    pickle.dump(K.eval(g_multihead_layer.kernel), f)\n",
    "with open(output_path + '/%s/train_bias.pickle' % (alg_name), 'wb') as f:\n",
    "    pickle.dump(K.eval(g_multihead_layer.bias), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7789eb4c-f934-4463-b815-ebf092c707f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 3. the propensity-harnessed learning (PHL) - step 1 & 2 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e36d1f-e3ae-4144-853a-3b734453297d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "alg_name = 'PHL'\n",
    "\n",
    "if not os.path.exists(output_path + '/' + alg_name):\n",
    "    os.mkdir(output_path + '/' + alg_name)        \n",
    "    \n",
    "for j_fold in range(n_folds):\n",
    "\n",
    "    for i_fold in range(2):\n",
    "\n",
    "        # select training and validation sets\n",
    "        train_pids = provider_ids[train_list[j_fold][i_fold]]\n",
    "        n_train_providers = len(train_pids)\n",
    "        train_inputs = df_feature.loc[train_pids]\n",
    "        train_labels = df_label.loc[train_pids]\n",
    "        train_flags = pd.concat([pd.DataFrame(index=df_feature.loc[pid].index, columns=[pid], data=1) for pid in train_pids]).fillna(0)\n",
    "        \n",
    "        valid_pids = provider_ids[valid_list[j_fold]]\n",
    "        valid_inputs = df_feature.loc[valid_pids]\n",
    "        valid_labels = df_label.loc[valid_pids]\n",
    "\n",
    "        print(j_fold, i_fold, train_pids, valid_pids, train_inputs.shape,  valid_inputs.shape)\n",
    "            \n",
    "        # step 1, train individual MLP models\n",
    "        train_fk_inputs = []\n",
    "        valid_fk_inputs = []\n",
    "            \n",
    "        for pid in train_pids:\n",
    "                \n",
    "            print('learning ind models, ', pid, end='\\r')\n",
    "                \n",
    "            ind_model = MLP_model(\n",
    "                params={'n_inputs': n_inputs,\n",
    "                        'n_labels': n_labels,\n",
    "                        'layer_shape': layer_shape,\n",
    "                        'common_reg': used_reg,}\n",
    "            ) \n",
    "\n",
    "            ind_model.fit(\n",
    "                [df_feature.loc[pid], df_label.loc[pid]], [], \n",
    "                batch_size=batch_size, \n",
    "                epochs=n_iters,\n",
    "                shuffle=True,\n",
    "                workers=40,\n",
    "                use_multiprocessing=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "                \n",
    "            train_fk_inputs.append(ind_model.predict([train_inputs, np.zeros(train_labels.shape)])[:, np.newaxis, :])\n",
    "            valid_fk_inputs.append(ind_model.predict([valid_inputs, np.zeros(valid_labels.shape)])[:, np.newaxis, :])\n",
    "                \n",
    "        train_fk_inputs = np.hstack(train_fk_inputs)\n",
    "        valid_fk_inputs = np.hstack(valid_fk_inputs)\n",
    "            \n",
    "        # step 2, train PHL model\n",
    "        PHL_model = PHL_model(\n",
    "                params={'n_inputs': n_inputs,\n",
    "                        'n_labels': n_labels,\n",
    "                        'layer_shape': layer_shape,\n",
    "                        'common_reg': used_reg,    \n",
    "                        'n_train_providers': n_train_providers,\n",
    "                        'gk_l1l2': gk_l1l2,\n",
    "                        'gb_l1l2': gb_l1l2},\n",
    "            w_pa=used_wpa,\n",
    "        ) \n",
    "            \n",
    "        print('learning PHL model')\n",
    "            \n",
    "        PHL_model.fit(\n",
    "            [train_inputs, train_fk_inputs, train_labels, train_flags], [], \n",
    "            batch_size=batch_size, \n",
    "            epochs=n_iters,\n",
    "            shuffle=True,\n",
    "            workers=40,\n",
    "            use_multiprocessing=True,\n",
    "            verbose=2,\n",
    "        )\n",
    "        \n",
    "        # prediction\n",
    "        train_pred = PHL_model.predict([train_inputs, \n",
    "                                         train_fk_inputs,\n",
    "                                         np.zeros(train_labels.shape),\n",
    "                                         np.zeros([train_labels.shape[0], n_train_providers])])\n",
    "            \n",
    "        train_f_outputs, train_g_outputs = train_pred\n",
    "            \n",
    "        valid_pred = PHL_model.predict([valid_inputs, \n",
    "                                         valid_fk_inputs,\n",
    "                                         np.zeros(valid_labels.shape),\n",
    "                                         np.zeros([valid_labels.shape[0], n_train_providers])])\n",
    "            \n",
    "        valid_f_outputs, valid_g_outputs = valid_pred\n",
    "            \n",
    "        # storage\n",
    "        with open(output_path + '/%s/train_pred_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "            pickle.dump(train_f_outputs, f)\n",
    "        with open(output_path + '/%s/train_pred_g_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "            pickle.dump(train_g_outputs, f)\n",
    "        with open(output_path + '/%s/pred_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:\n",
    "            pickle.dump(valid_f_outputs, f)\n",
    "        with open(output_path + '/%s/pred_g_valid_%d_split_%d.pickle' % (alg_name, j_fold, i_fold), 'wb') as f:        \n",
    "            pickle.dump(valid_g_outputs, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f3e66-dff1-48a6-a981-5b7b0062c868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
